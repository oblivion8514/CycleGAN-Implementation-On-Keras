{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of CycleGAN <br>\n",
    "Project page: https://junyanz.github.io/CycleGAN/ <br>\n",
    "Step: <br>\n",
    "1. Download dataset from https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/\n",
    "2. Make new folder named 'data'\n",
    "3. Put the dataset into data folder\n",
    "4. Modify the hyperparameters and Run the 1st~5th cells in order\n",
    "5. After (4), you can generate current output by running the 6th cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras_contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-44bdca0048fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUpSampling2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mZeroPadding2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConv2DTranspose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAveragePooling2D\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mInputSpec\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras_contrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalization\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minstancenormalization\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInstanceNormalization\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madvanced_activations\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLeakyReLU\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras_contrib'"
     ]
    }
   ],
   "source": [
    "from keras.layers import Layer, Input, Conv2D, Activation, add, BatchNormalization, UpSampling2D, ZeroPadding2D, Conv2DTranspose, Flatten, MaxPooling2D, AveragePooling2D,InputSpec\n",
    "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
    "\n",
    "\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.backend import mean\n",
    "from keras.models import Model, model_from_json\n",
    "from keras.utils import plot_model\n",
    "from keras.engine.topology import Network\n",
    "\n",
    "from collections import OrderedDict\n",
    "from scipy.misc import imsave, toimage  \n",
    "import numpy as np\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "from PIL import Image\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(seed=1111)\n",
    "\n",
    "class CycleGAN():\n",
    "    def __init__(self, lr_D=2e-4, lr_G=2e-4, image_shape=(256, 256, 3),\n",
    "                 date_time_string_addition='_test', image_folder='apple2orange'):\n",
    "        self.img_shape = image_shape\n",
    "        self.channels = 3\n",
    "        self.normalization = InstanceNormalization\n",
    "        # Hyper parameters\n",
    "        self.lambda_C = 10.0 # weight for Cycle-Consistency loss\n",
    "        self.lambda_D = 1.0  # Weight for DA,DB\n",
    "        self.learning_rate_D = lr_D\n",
    "        self.learning_rate_G = lr_G\n",
    "        self.generator_iterations = 1 \n",
    "        self.discriminator_iterations = 1  \n",
    "        self.beta_1 = 0.5\n",
    "        self.beta_2 = 0.999\n",
    "        self.batch_size = 1\n",
    "        self.epochs = 100 \n",
    "        self.save_interval = 1\n",
    "        self.synthetic_pool_size = 50\n",
    "        #special options : identity learning to avoid opposite output of background (non_ROI regions)\n",
    "        self.id_learning=False\n",
    "        self.id_weight=0.5\n",
    "        \n",
    "        self.REAL_LABEL = 1.0\n",
    "        # Used as storage folder name\n",
    "        self.date_time = time.strftime('%Y%m%d-%H%M%S', time.localtime()) + date_time_string_addition\n",
    "        \n",
    "        #optimizer\n",
    "        self.opt_D = Adam(self.learning_rate_D, self.beta_1, self.beta_2)\n",
    "        self.opt_G = Adam(self.learning_rate_G, self.beta_1, self.beta_2)\n",
    "       \n",
    "        D_A = self.Build_D()\n",
    "        D_B = self.Build_D()\n",
    "        loss_weights_D = [0.5]  \n",
    "\n",
    "        # DA,DB build up\n",
    "        image_A = Input(shape=self.img_shape)\n",
    "        image_B = Input(shape=self.img_shape)\n",
    "        guess_A = D_A(image_A)\n",
    "        guess_B = D_B(image_B)\n",
    "        self.D_A = Model(inputs=image_A, outputs=guess_A, name='D_A_model')\n",
    "        self.D_B = Model(inputs=image_B, outputs=guess_B, name='D_B_model')\n",
    "        self.D_A.compile(optimizer=self.opt_D,\n",
    "                         loss=self.lse,\n",
    "                         loss_weights=loss_weights_D)\n",
    "        self.D_B.compile(optimizer=self.opt_D,\n",
    "                         loss=self.lse,\n",
    "                         loss_weights=loss_weights_D)\n",
    "\n",
    "        \n",
    "        self.D_A_static = Network(inputs=image_A, outputs=guess_A, name='D_A_static_model')\n",
    "        self.D_B_static = Network(inputs=image_B, outputs=guess_B, name='D_B_static_model')\n",
    "        self.D_A_static.trainable = False\n",
    "        self.D_B_static.trainable = False\n",
    "        \"\"\"\n",
    "        without static Ds, the Ds in the G_model may be trainable even after setting the trainable var to False\n",
    "        \"\"\"\n",
    "\n",
    "        # Generators build up\n",
    "        self.G_A2B = self.Build_G(name='G_A2B_model')\n",
    "        self.G_B2A = self.Build_G(name='G_B2A_model')\n",
    "\n",
    "        real_A = Input(shape=self.img_shape, name='real_A')\n",
    "        real_B = Input(shape=self.img_shape, name='real_B')\n",
    "        synthetic_B = self.G_A2B(real_A)\n",
    "        synthetic_A = self.G_B2A(real_B)\n",
    "        dA_guess_synthetic = self.D_A_static(synthetic_A)\n",
    "        dB_guess_synthetic = self.D_B_static(synthetic_B)\n",
    "        reconstructed_A = self.G_B2A(synthetic_B)\n",
    "        reconstructed_B = self.G_A2B(synthetic_A)\n",
    "        if(self.id_learning):\n",
    "            identity_A=self.G_A2B(real_A)\n",
    "            identity_B=self.G_B2A(real_B)\n",
    "        \n",
    "        model_outputs = [reconstructed_A, reconstructed_B]\n",
    "        model_outputs.append(dA_guess_synthetic)\n",
    "        model_outputs.append(dB_guess_synthetic)\n",
    "        if(self.id_learning):\n",
    "            model_outputs.append(identity_A)\n",
    "            model_outputs.append(identity_B)\n",
    "\n",
    "            compile_losses = ['mae','mae','mse','mse','mae','mae']\n",
    "            compile_weights = [self.lambda_C, self.lambda_C,\n",
    "                               self.lambda_D, self.lambda_D,\n",
    "                               self.lambda_C*self.id_weight,self.lambda_C*self.id_weight]\n",
    "        else:\n",
    "            compile_losses = ['mae','mae','mse','mse']\n",
    "            compile_weights = [self.lambda_C, self.lambda_C,\n",
    "                           self.lambda_D, self.lambda_D]\n",
    "\n",
    "        self.G_model = Model(inputs=[real_A, real_B],\n",
    "                             outputs=model_outputs,\n",
    "                             name='G_model')\n",
    "\n",
    "        self.G_model.compile(optimizer=self.opt_G,\n",
    "                             loss=compile_losses,\n",
    "                             loss_weights=compile_weights)\n",
    "        # self.G_A2B.summary()\n",
    "\n",
    "        # Data    \n",
    "        print('--- Caching datasets---')\n",
    "        # Data Loading\n",
    "        def load_data(nr_of_channels=3, batch_size=1, subfolder=''):\n",
    "            trainA_path = os.path.join('data', subfolder, 'trainA')\n",
    "            trainB_path = os.path.join('data', subfolder, 'trainB')\n",
    "            testA_path = os.path.join('data', subfolder, 'testA')\n",
    "            testB_path = os.path.join('data', subfolder, 'testB')\n",
    "            trainA_image_names=os.listdir(trainA_path)\n",
    "            trainB_image_names=os.listdir(trainB_path)\n",
    "            testA_image_names=os.listdir(testA_path)\n",
    "            testB_image_names=os.listdir(testB_path)\n",
    "            trainA_images = create_image_array(trainA_image_names, trainA_path, nr_of_channels)\n",
    "            trainB_images = create_image_array(trainB_image_names, trainB_path, nr_of_channels)\n",
    "            testA_images = create_image_array(testA_image_names, testA_path, nr_of_channels)\n",
    "            testB_images = create_image_array(testB_image_names, testB_path, nr_of_channels)\n",
    "            return {\"trainA_images\": trainA_images, \"trainB_images\": trainB_images,\n",
    "                    \"testA_images\": testA_images, \"testB_images\": testB_images,\n",
    "                    \"trainA_image_names\": trainA_image_names,\n",
    "                    \"trainB_image_names\": trainB_image_names,\n",
    "                    \"testA_image_names\": testA_image_names,\n",
    "                    \"testB_image_names\": testB_image_names}\n",
    "        def create_image_array(image_list, image_path, nr_of_channels):\n",
    "            image_array = []\n",
    "            for image_name in image_list:\n",
    "                if image_name[-1].lower() == 'g':  # to avoid e.g. thumbs.db files\n",
    "                    if nr_of_channels == 1:  # Gray scale image -> MR image\n",
    "                        image = np.array(Image.open(os.path.join(image_path, image_name)))\n",
    "                        image = image[:, :, np.newaxis]\n",
    "                    else:                   # RGB image -> street view\n",
    "                        image = np.array(Image.open(os.path.join(image_path, image_name)))\n",
    "                        if(np.array(image).shape==(256,256)):\n",
    "                            image=np.stack((image,)*3,axis=-1)\n",
    "                    image = normalize_array(image)\n",
    "                    image_array.append(image)\n",
    "\n",
    "            return np.array(image_array)\n",
    "\n",
    "        # convert 0~255 to -1~1\n",
    "        def normalize_array(array):\n",
    "            array = array / 127.5 - 1\n",
    "            return array\n",
    "\n",
    "\n",
    "        data =load_data(nr_of_channels=self.channels,\n",
    "                           batch_size=self.batch_size,\n",
    "                           subfolder=image_folder)\n",
    "\n",
    "        self.A_train = data[\"trainA_images\"]\n",
    "        self.B_train = data[\"trainB_images\"]\n",
    "        self.A_test = data[\"testA_images\"]\n",
    "        self.B_test = data[\"testB_images\"]\n",
    "        self.testA_image_names = data[\"testA_image_names\"]\n",
    "        self.testB_image_names = data[\"testB_image_names\"]\n",
    "\n",
    "\n",
    "        # Create run folder and store meta data\n",
    "        directory = os.path.join('images', self.date_time)\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        self.writeMetaDataToJSON()\n",
    "\n",
    "        config = tf.ConfigProto()\n",
    "\n",
    "        config.gpu_options.allow_growth = True\n",
    "\n",
    "        K.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "# Layers help functions\n",
    "    def ck(self, x, k, use_normalization):\n",
    "        x = Conv2D(filters=k, kernel_size=4, strides=2, padding='same')(x)\n",
    "        # Normalization is not done on the first discriminator layer\n",
    "        if use_normalization:\n",
    "            x = self.normalization(axis=3, center=True, epsilon=1e-5)(x, training=True)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "        return x\n",
    "    def c7Ak(self, x, k):\n",
    "        x = Conv2D(filters=k, kernel_size=7, strides=1, padding='valid')(x)\n",
    "        x = self.normalization(axis=3, center=True, epsilon=1e-5)(x, training=True)\n",
    "        x = Activation('relu')(x)\n",
    "        return x\n",
    "    def dk(self, x, k):\n",
    "        x = Conv2D(filters=k, kernel_size=3, strides=2, padding='same')(x)\n",
    "        x = self.normalization(axis=3, center=True, epsilon=1e-5)(x, training=True)\n",
    "        x = Activation('relu')(x)\n",
    "        return x\n",
    "    def Rk(self, x0):\n",
    "        k = int(x0.shape[-1])\n",
    "        # first layer\n",
    "        x = ReflectionPadding2D((1,1))(x0)\n",
    "        x = Conv2D(filters=k, kernel_size=3, strides=1, padding='valid')(x)\n",
    "        x = self.normalization(axis=3, center=True, epsilon=1e-5)(x, training=True)\n",
    "        x = Activation('relu')(x)\n",
    "        # second layer\n",
    "        x = ReflectionPadding2D((1, 1))(x)\n",
    "        x = Conv2D(filters=k, kernel_size=3, strides=1, padding='valid')(x)\n",
    "        x = self.normalization(axis=3, center=True, epsilon=1e-5)(x, training=True)\n",
    "        # merge\n",
    "        x = add([x, x0])\n",
    "        return x\n",
    "    def uk(self, x, k):\n",
    "        x = Conv2DTranspose(filters=k, kernel_size=3, strides=2, padding='same')(x)  # this matches fractinoally stided with stride 1/2\n",
    "        x = self.normalization(axis=3, center=True, epsilon=1e-5)(x, training=True)\n",
    "        x = Activation('relu')(x)\n",
    "        return x\n",
    "# Models helper function\n",
    "    def Build_D(self, name=None):\n",
    "        # Specify input\n",
    "        input_img = Input(shape=self.img_shape)\n",
    "        x = self.ck(input_img, 64, False)\n",
    "        x = self.ck(x, 128, True)\n",
    "        x = self.ck(x, 256, True)\n",
    "        x = self.ck(x, 512, True)      \n",
    "        x = Conv2D(filters=1, kernel_size=4, strides=1, padding='same')(x)\n",
    "        x = Activation('sigmoid')(x)\n",
    "        return Model(inputs=input_img, outputs=x, name=name)\n",
    "    def Build_G(self, name=None):\n",
    "        # Specify input\n",
    "        input_img = Input(shape=self.img_shape)\n",
    "        # convolution\n",
    "        x = ReflectionPadding2D((3, 3))(input_img)\n",
    "        x = self.c7Ak(x, 32)\n",
    "        x = self.dk(x, 64)\n",
    "        x = self.dk(x, 128)\n",
    "        # Residual blocks\n",
    "        for _ in range(4, 13):\n",
    "            x = self.Rk(x)\n",
    "        # up-sampling\n",
    "        x = self.uk(x, 64)\n",
    "        x = self.uk(x, 32)\n",
    "        x = ReflectionPadding2D((3, 3))(x)\n",
    "        x = Conv2D(self.channels, kernel_size=7, strides=1)(x)\n",
    "        x = Activation('tanh')(x)  \n",
    "        return Model(inputs=input_img, outputs=x, name=name)\n",
    "# Training\n",
    "    def train(self, epochs, batch_size=1, save_interval=1):\n",
    "        def run_training_iteration(loop_index, epoch_iterations):\n",
    "            # Discriminator training \n",
    "            synthetic_images_B = self.G_A2B.predict(real_images_A)\n",
    "            synthetic_images_A = self.G_B2A.predict(real_images_B)\n",
    "            synthetic_images_A = synthetic_pool_A.query(synthetic_images_A)\n",
    "            synthetic_images_B = synthetic_pool_B.query(synthetic_images_B)\n",
    "\n",
    "            for _ in range(self.discriminator_iterations):\n",
    "                DA_loss_real = self.D_A.train_on_batch(x=real_images_A, y=ones)\n",
    "                DB_loss_real = self.D_B.train_on_batch(x=real_images_B, y=ones)\n",
    "                DA_loss_synthetic = self.D_A.train_on_batch(x=synthetic_images_A, y=zeros)\n",
    "                DB_loss_synthetic = self.D_B.train_on_batch(x=synthetic_images_B, y=zeros)\n",
    "                \n",
    "                DA_loss = DA_loss_real + DA_loss_synthetic\n",
    "                DB_loss = DB_loss_real + DB_loss_synthetic\n",
    "                D_loss = DA_loss + DB_loss\n",
    "\n",
    "                if self.discriminator_iterations > 1:\n",
    "                    print('D_loss:', D_loss)\n",
    "                    sys.stdout.flush()\n",
    "\n",
    "            # Generator training \n",
    "            target_data = [real_images_A, real_images_B]  # Compare reconstructed images to real images\n",
    "            \n",
    "            target_data.append(ones)\n",
    "            target_data.append(ones)\n",
    "            if(self.id_learning):\n",
    "                target_data.append(real_images_A)\n",
    "                target_data.append(real_images_B)            \n",
    "\n",
    "            for _ in range(self.generator_iterations):\n",
    "                G_loss = self.G_model.train_on_batch(\n",
    "                    x=[real_images_A, real_images_B], y=target_data)\n",
    "                if self.generator_iterations > 1:\n",
    "                    print('G_loss:', G_loss)\n",
    "                    sys.stdout.flush()\n",
    "            reconstruction_loss_A = G_loss[1]\n",
    "            reconstruction_loss_B = G_loss[2]\n",
    "            gA_d_loss_synthetic = G_loss[3]\n",
    "            gB_d_loss_synthetic = G_loss[4]\n",
    "            if(self.id_learning):\n",
    "                identity_loss_A=G_loss[5]\n",
    "                identity_loss_B=G_loss[6]\n",
    "            \n",
    "            # Store training data\n",
    "            DA_losses.append(DA_loss)\n",
    "            DB_losses.append(DB_loss)\n",
    "            gA_d_losses_synthetic.append(gA_d_loss_synthetic)\n",
    "            gB_d_losses_synthetic.append(gB_d_loss_synthetic)\n",
    "            gA_losses_reconstructed.append(reconstruction_loss_A)\n",
    "            gB_losses_reconstructed.append(reconstruction_loss_B)\n",
    "\n",
    "            GA_loss = gA_d_loss_synthetic + reconstruction_loss_A\n",
    "            GB_loss = gB_d_loss_synthetic + reconstruction_loss_B\n",
    "            if(self.id_learning):\n",
    "                GA_loss+=identity_loss_A\n",
    "                GB_loss+=identity_loss_B\n",
    "            D_losses.append(D_loss)\n",
    "            GA_losses.append(GA_loss)\n",
    "            GB_losses.append(GB_loss)\n",
    "            G_losses.append(G_loss)\n",
    "            reconstruction_loss = reconstruction_loss_A + reconstruction_loss_B\n",
    "            reconstruction_losses.append(reconstruction_loss)\n",
    "            clear_output()\n",
    "            print('\\n')\n",
    "            print('Epoch----------------', epoch, '/', epochs)\n",
    "            print('Loop index----------------', loop_index + 1, '/', epoch_iterations)\n",
    "            print('D_loss: ', D_loss)\n",
    "            print('G_loss: ', G_loss[0])\n",
    "            print('reconstruction_loss: ', reconstruction_loss)\n",
    "            print('dA_loss:', DA_loss)\n",
    "            print('DB_loss:', DB_loss)\n",
    "\n",
    "            if loop_index % 20 == 0:\n",
    "                # Save temporary images \n",
    "                self.save_tmp_images(real_images_A, real_images_B, synthetic_images_A, synthetic_images_B)\n",
    "                #self.print_ETA(start_time, epoch, epoch_iterations, loop_index)\n",
    "\n",
    "        # Begin training\n",
    "        training_history = OrderedDict()\n",
    "\n",
    "        DA_losses = []\n",
    "        DB_losses = []\n",
    "        gA_d_losses_synthetic = []\n",
    "        gB_d_losses_synthetic = []\n",
    "        gA_losses_reconstructed = []\n",
    "        gB_losses_reconstructed = []\n",
    "\n",
    "        GA_losses = []\n",
    "        GB_losses = []\n",
    "        reconstruction_losses = []\n",
    "        D_losses = []\n",
    "        G_losses = []\n",
    "\n",
    "        # Image pools used to update the discriminators\n",
    "        synthetic_pool_A = ImagePool(self.synthetic_pool_size)\n",
    "        synthetic_pool_B = ImagePool(self.synthetic_pool_size)\n",
    "        \n",
    "        label_shape = (batch_size,) + self.D_A.output_shape[1:]\n",
    "        ones = np.ones(shape=label_shape) * self.REAL_LABEL\n",
    "        zeros = ones * 0\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            A_train = self.A_train\n",
    "            B_train = self.B_train\n",
    "            # if train dataset>2000,only train 2000 iteration for each epoch\n",
    "            epoch_iterations=min(2000,max(len(A_train),len(B_train)))\n",
    "            random_order_A = np.random.randint(len(A_train), size=epoch_iterations)\n",
    "            random_order_B = np.random.randint(len(B_train), size=epoch_iterations)\n",
    "            \n",
    "            print(\"epoch: \",epoch,\" iterations: \",epoch_iterations)\n",
    "\n",
    "\n",
    "            for loop_index in range(0, epoch_iterations,batch_size):\n",
    "                index_A=random_order_A[loop_index:loop_index+batch_size]\n",
    "                index_B=random_order_B[loop_index:loop_index+batch_size]\n",
    "\n",
    "                sys.stdout.flush()\n",
    "                real_images_A = A_train[index_A]\n",
    "                real_images_B = B_train[index_B]\n",
    "\n",
    "                # Run all training steps\n",
    "                run_training_iteration(loop_index, epoch_iterations)\n",
    "\n",
    "            print('\\n')\n",
    "            print('Epoch----------------', epoch, '/', epochs)\n",
    "            print('D_loss: ',D_losses[-1])\n",
    "            print('G_loss: ',G_losses[-1][0])\n",
    "            print('reconstructed loss: ',reconstruction_losses[-1])\n",
    "            print('DA_loss: ',DA_losses[-1])\n",
    "            print('DB_loss: ',DB_losses[-1])\n",
    "            if epoch % save_interval == 0:\n",
    "                print('\\n', '\\n', '-------------------------Saving images for epoch', epoch, '-------------------------', '\\n', '\\n')\n",
    "                self.saveImages(epoch, real_images_A, real_images_B)\n",
    "\n",
    "            if epoch % 20 == 0:\n",
    "                \n",
    "                self.saveModel(self.D_A, epoch)\n",
    "                self.saveModel(self.D_B, epoch)\n",
    "                self.saveModel(self.G_A2B, epoch)\n",
    "                self.saveModel(self.G_B2A, epoch)\n",
    "\n",
    "            training_history = {\n",
    "                'DA_losses': DA_losses,\n",
    "                'DB_losses': DB_losses,\n",
    "                'gA_d_losses_synthetic': gA_d_losses_synthetic,\n",
    "                'gB_d_losses_synthetic': gB_d_losses_synthetic,\n",
    "                'gA_losses_reconstructed': gA_losses_reconstructed,\n",
    "                'gB_losses_reconstructed': gB_losses_reconstructed,\n",
    "                'D_losses': D_losses,\n",
    "                'G_losses': G_losses,\n",
    "                'reconstruction_losses': reconstruction_losses}\n",
    "            self.writeLossDataToFile(training_history)\n",
    "\n",
    "            sys.stdout.flush()\n",
    "\n",
    "# other helper functions\n",
    "    def lse(self, y_true, y_pred):\n",
    "        loss = tf.reduce_mean(tf.squared_difference(y_pred, y_true))\n",
    "        return loss\n",
    "    def truncateAndSave(self, real_, real, synthetic, reconstructed, path_name):\n",
    "        if len(real.shape) > 3:\n",
    "            real = real[0]\n",
    "            synthetic = synthetic[0]\n",
    "            reconstructed = reconstructed[0]\n",
    "\n",
    "        # Append and save\n",
    "        if real_ is not None:\n",
    "            if len(real_.shape) > 4:\n",
    "                real_ = real_[0]\n",
    "            image = np.hstack((real_[0], real, synthetic, reconstructed))\n",
    "        else:\n",
    "            image = np.hstack((real, synthetic, reconstructed))\n",
    "\n",
    "        if self.channels == 1:\n",
    "            image = image[:, :, 0]\n",
    "\n",
    "        toimage(image, cmin=-1, cmax=1).save(path_name)\n",
    "\n",
    "    def saveImages(self, epoch, real_image_A, real_image_B, num_saved_images=1):\n",
    "        directory = os.path.join('images', self.date_time)\n",
    "        if not os.path.exists(os.path.join(directory, 'A')):\n",
    "            os.makedirs(os.path.join(directory, 'A'))\n",
    "            os.makedirs(os.path.join(directory, 'B'))\n",
    "            os.makedirs(os.path.join(directory, 'Atest'))\n",
    "            os.makedirs(os.path.join(directory, 'Btest'))\n",
    "\n",
    "        testString = ''\n",
    "\n",
    "        real_image_Ab = None\n",
    "        real_image_Ba = None\n",
    "        for i in range(num_saved_images + 1):\n",
    "            if i == num_saved_images:\n",
    "                real_image_A = self.A_test[0]\n",
    "                real_image_B = self.B_test[0]\n",
    "                real_image_A = np.expand_dims(real_image_A, axis=0)\n",
    "                real_image_B = np.expand_dims(real_image_B, axis=0)\n",
    "                testString = 'test'\n",
    "                \n",
    "            else:\n",
    "                #real_image_A = self.A_train[rand_A_idx[i]]\n",
    "                #real_image_B = self.B_train[rand_B_idx[i]]\n",
    "                if len(real_image_A.shape) < 4:\n",
    "                    real_image_A = np.expand_dims(real_image_A, axis=0)\n",
    "                    real_image_B = np.expand_dims(real_image_B, axis=0)\n",
    "                \n",
    "            synthetic_image_B = self.G_A2B.predict(real_image_A)\n",
    "            synthetic_image_A = self.G_B2A.predict(real_image_B)\n",
    "            reconstructed_image_A = self.G_B2A.predict(synthetic_image_B)\n",
    "            reconstructed_image_B = self.G_A2B.predict(synthetic_image_A)\n",
    "\n",
    "            self.truncateAndSave(real_image_Ab, real_image_A, synthetic_image_B, reconstructed_image_A,\n",
    "                                 'images/{}/{}/epoch{}_sample{}.png'.format(\n",
    "                                     self.date_time, 'A' + testString, epoch, i))\n",
    "            self.truncateAndSave(real_image_Ba, real_image_B, synthetic_image_A, reconstructed_image_B,\n",
    "                                 'images/{}/{}/epoch{}_sample{}.png'.format(\n",
    "                                     self.date_time, 'B' + testString, epoch, i))\n",
    "\n",
    "    def save_tmp_images(self, real_image_A, real_image_B, synthetic_image_A, synthetic_image_B):\n",
    "        try:\n",
    "            reconstructed_image_A = self.G_B2A.predict(synthetic_image_B)\n",
    "            reconstructed_image_B = self.G_A2B.predict(synthetic_image_A)\n",
    "\n",
    "            real_images = np.vstack((real_image_A[0], real_image_B[0]))\n",
    "            synthetic_images = np.vstack((synthetic_image_B[0], synthetic_image_A[0]))\n",
    "            reconstructed_images = np.vstack((reconstructed_image_A[0], reconstructed_image_B[0]))\n",
    "\n",
    "            self.truncateAndSave(None, real_images, synthetic_images, reconstructed_images,\n",
    "                                 'images/{}/{}.png'.format(\n",
    "                                     self.date_time, 'tmp'))\n",
    "        except: # Ignore if file is open\n",
    "            pass\n",
    "\n",
    "\n",
    "    def print_ETA(self, start_time, epoch, epoch_iterations, loop_index):\n",
    "        passed_time = time.time() - start_time\n",
    "\n",
    "        iterations_so_far = ((epoch - 1) * epoch_iterations + loop_index) / self.batch_size\n",
    "        iterations_total = self.epochs * epoch_iterations / self.batch_size\n",
    "        iterations_left = iterations_total - iterations_so_far\n",
    "        eta = round(passed_time / (iterations_so_far + 1e-5) * iterations_left)\n",
    "\n",
    "        passed_time_string = str(datetime.timedelta(seconds=round(passed_time)))\n",
    "        eta_string = str(datetime.timedelta(seconds=eta))\n",
    "        print('Time passed', passed_time_string, ': ETA in', eta_string)\n",
    "\n",
    "# Save and load\n",
    "    def saveModel(self, model, epoch):\n",
    "        # Create folder to save model architecture and weights\n",
    "        directory = os.path.join('saved_models', self.date_time)\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "        model_path_w = 'saved_models/{}/{}_weights_epoch_{}.hdf5'.format(self.date_time, model.name, epoch)\n",
    "        model.save_weights(model_path_w)\n",
    "        model_path_m = 'saved_models/{}/{}_model_epoch_{}.json'.format(self.date_time, model.name, epoch)\n",
    "        model.save_weights(model_path_m)\n",
    "        json_string = model.to_json()\n",
    "        with open(model_path_m, 'w') as outfile:\n",
    "            json.dump(json_string, outfile)\n",
    "        print('{} has been saved in saved_models/{}/'.format(model.name, self.date_time))\n",
    "\n",
    "    def writeLossDataToFile(self, history):\n",
    "        keys = sorted(history.keys())\n",
    "        with open('images/{}/loss_output.csv'.format(self.date_time), 'w') as csv_file:\n",
    "            writer = csv.writer(csv_file, delimiter=',')\n",
    "            writer.writerow(keys)\n",
    "            writer.writerows(zip(*[history[key] for key in keys]))\n",
    "\n",
    "    def writeMetaDataToJSON(self):\n",
    "\n",
    "        directory = os.path.join('images', self.date_time)\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        # Save meta_data\n",
    "        data = {}\n",
    "        data['meta_data'] = []\n",
    "        data['meta_data'].append({\n",
    "            'img shape: height,width,channels': self.img_shape,\n",
    "            'batch size': self.batch_size,\n",
    "            'save interval': self.save_interval,\n",
    "            'normalization function': str(self.normalization),\n",
    "            'lambda': self.lambda_C,\n",
    "            'lambda': self.lambda_C,\n",
    "            'lambda_d': self.lambda_D,\n",
    "            'learning_rate_D': self.learning_rate_D,\n",
    "            'learning rate G': self.learning_rate_G,\n",
    "            'epochs': self.epochs,\n",
    "            'generator iterations': self.generator_iterations,\n",
    "            'discriminator iterations': self.discriminator_iterations,\n",
    "            'beta 1': self.beta_1,\n",
    "            'beta 2': self.beta_2,\n",
    "            'REAL_LABEL': self.REAL_LABEL,\n",
    "            'number of A train examples': len(self.A_train),\n",
    "            'number of B train examples': len(self.B_train),\n",
    "            'number of A test examples': len(self.A_test),\n",
    "            'number of B test examples': len(self.B_test),\n",
    "        })\n",
    "\n",
    "        with open('images/{}/meta_data.json'.format(self.date_time), 'w') as outfile:\n",
    "            json.dump(data, outfile, sort_keys=True)\n",
    "\n",
    "    def load_model_and_weights(self, model):\n",
    "        path_to_model = os.path.join('generate_images', 'models', '{}.json'.format(model.name))\n",
    "        path_to_weights = os.path.join('generate_images', 'models', '{}.hdf5'.format(model.name))\n",
    "        #model = model_from_json(path_to_model)\n",
    "        model.load_weights(path_to_weights)\n",
    "\n",
    "    def load_model_and_generate_synthetic_images(self):\n",
    "        response = input('Are you sure you want to generate synthetic images instead of training? (y/n): ')[0].lower()\n",
    "        if response == 'y':\n",
    "            self.load_model_and_weights(self.G_A2B)\n",
    "            self.load_model_and_weights(self.G_B2A)\n",
    "            synthetic_images_B = self.G_A2B.predict(self.A_test)\n",
    "            synthetic_images_A = self.G_B2A.predict(self.B_test)\n",
    "\n",
    "            def save_image(image, name, domain):\n",
    "                if self.channels == 1:\n",
    "                    image = image[:, :, 0]\n",
    "                toimage(image, cmin=-1, cmax=1).save(os.path.join(\n",
    "                    'generate_images', 'synthetic_images', domain, name))\n",
    "\n",
    "            # Test A images\n",
    "            for i in range(len(synthetic_images_A)):\n",
    "                # Get the name from the image it was conditioned on\n",
    "                name = self.testB_image_names[i].strip('.png') + '_synthetic.png'\n",
    "                synt_A = synthetic_images_A[i]\n",
    "                save_image(synt_A, name, 'A')\n",
    "\n",
    "            # Test B images\n",
    "            for i in range(len(synthetic_images_B)):\n",
    "                # Get the name from the image it was conditioned on\n",
    "                name = self.testA_image_names[i].strip('.png') + '_synthetic.png'\n",
    "                synt_B = synthetic_images_B[i]\n",
    "                save_image(synt_B, name, 'B')\n",
    "\n",
    "            print('{} synthetic images have been generated and placed in ./generate_images/synthetic_images'\n",
    "                  .format(len(self.A_test) + len(self.B_test)))\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/questions/50677544/reflection-padding-conv2d\n",
    "class ReflectionPadding2D(Layer):\n",
    "    def __init__(self, padding=(1, 1), **kwargs):\n",
    "        self.padding = tuple(padding)\n",
    "        self.input_spec = [InputSpec(ndim=4)]\n",
    "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, s):\n",
    "        return (s[0], s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1], s[3])\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        w_pad, h_pad = self.padding\n",
    "        return tf.pad(x, [[0, 0], [h_pad, h_pad], [w_pad, w_pad], [0, 0]], 'REFLECT')\n",
    "\n",
    "\n",
    "class ImagePool():\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "        if self.pool_size > 0:\n",
    "            self.num_imgs = 0\n",
    "            self.images = []\n",
    "\n",
    "    def query(self, images):\n",
    "        if self.pool_size == 0:\n",
    "            return images\n",
    "        return_images = []\n",
    "        for image in images:\n",
    "            if len(image.shape) == 3:\n",
    "                image = image[np.newaxis, :, :, :]\n",
    "\n",
    "            if self.num_imgs < self.pool_size:  # fill up the image pool\n",
    "                self.num_imgs = self.num_imgs + 1\n",
    "                if len(self.images) == 0:\n",
    "                    self.images = image\n",
    "                else:\n",
    "                    self.images = np.vstack((self.images, image))\n",
    "\n",
    "                if len(return_images) == 0:\n",
    "                    return_images = image\n",
    "                else:\n",
    "                    return_images = np.vstack((return_images, image))\n",
    "\n",
    "            else:  # 50% chance that replace an old synthetic image\n",
    "                p = random.uniform(0, 1)\n",
    "                if p > 0.5:\n",
    "                    random_id = random.randint(0, self.pool_size - 1)\n",
    "                    tmp = self.images[random_id, :, :, :]\n",
    "                    tmp = tmp[np.newaxis, :, :, :]\n",
    "                    self.images[random_id, :, :, :] = image[0, :, :, :]\n",
    "                    if len(return_images) == 0:\n",
    "                        return_images = tmp\n",
    "                    else:\n",
    "                        return_images = np.vstack((return_images, tmp))\n",
    "                else:\n",
    "                    if len(return_images) == 0:\n",
    "                        return_images = image\n",
    "                    else:\n",
    "                        return_images = np.vstack((return_images, image))\n",
    "\n",
    "        return return_images\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAN = CycleGAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "GAN.G_A2B.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_D_model=GAN.Build_D()\n",
    "test_D_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAN.train(epochs=GAN.epochs, batch_size=GAN.batch_size, save_interval=GAN.save_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "if not os.path.exists('output/'):\n",
    "    os.makedirs('output')\n",
    "    \n",
    "if not os.path.exists('output/trainA2B/'):\n",
    "    os.makedirs('output/trainA2B')\n",
    "    os.makedirs('output/testA2B')\n",
    "    os.makedirs('output/trainB2A')\n",
    "    os.makedirs('output/testB2A')\n",
    "    \n",
    "for i in range(0,len(GAN.A_train)):\n",
    "    i_A=((GAN.A_train[i]+1)/2).reshape(1,256,256,3)\n",
    "    i_B=(GAN.G_A2B.predict(i_A).reshape(256,256,3)+1)/2\n",
    "    i_append=np.append(i_A.reshape(256,256,3),i_B,axis=1)\n",
    "    plt.imsave(\"output/trainA2B/train%s\"%i,i_append)\n",
    "    \n",
    "for i in range(0,len(GAN.A_test)):\n",
    "    i_A=((GAN.A_test[i]+1)/2).reshape(1,256,256,3)\n",
    "    i_B=(GAN.G_A2B.predict(i_A).reshape(256,256,3)+1)/2\n",
    "    i_append=np.append(i_A.reshape(256,256,3),i_B,axis=1)\n",
    "    plt.imsave(\"output/testA2B/test%s\"%i,i_append)\n",
    "    \n",
    "for i in range(0,len(GAN.B_train)):\n",
    "    i_B=((GAN.B_train[i]+1)/2).reshape(1,256,256,3)\n",
    "    i_A=(GAN.G_B2A.predict(i_B).reshape(256,256,3)+1)/2\n",
    "    i_append=np.append(i_B.reshape(256,256,3),i_A,axis=1)\n",
    "    plt.imsave(\"output/trainB2A/train%s\"%i,i_append)\n",
    "    \n",
    "for i in range(0,len(GAN.B_test)):\n",
    "    i_B=((GAN.B_test[i]+1)/2).reshape(1,256,256,3)\n",
    "    i_A=(GAN.G_B2A.predict(i_B).reshape(256,256,3)+1)/2\n",
    "    i_append=np.append(i_B.reshape(256,256,3),i_A,axis=1)\n",
    "    plt.imsave(\"output/testB2A/test%s\"%i,i_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
